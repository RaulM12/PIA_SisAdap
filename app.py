# app.py
import gradio as gr
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import os # Importamos os para verificar la ruta

# --- 1. Cargar el Modelo Entrenado ---
MODEL_PATH = "./modelo_guia_mty_final"

# Verificamos si el modelo existe ANTES de intentar cargarlo
if not os.path.exists(MODEL_PATH):
    print("="*50)
    print(f"游뚿 Error: No se encontr칩 la carpeta del modelo en: {MODEL_PATH}")
    print("Por favor, ejecuta primero el script 'train.py' para entrenar y guardar el modelo.")
    print("Comando: python3 train.py")
    print("="*50)
    exit() # Detiene la ejecuci칩n si no hay modelo

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
except Exception as e:
    print(f"Error al cargar el modelo: {e}")
    exit()

# Configurar el pipeline
generator = pipeline(
    'text-generation',
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1 # Usar치 GPU si est치 disponible
)

# --- 2. Funciones del Chatbot ---

def generate_response(prompt):
    """Genera una respuesta de gu칤a a partir del prompt."""
    
    # Formato de prompt
    formatted_prompt = f"Usuario: {prompt} Gu칤a:"

    try:
        output = generator(
            formatted_prompt,
            max_new_tokens=150,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id
        )

        response_text = output[0]['generated_text']

        # Limpieza: Extraer solo la parte de la respuesta de la Gu칤a
        start_index = response_text.rfind("Gu칤a:") + len("Gu칤a:")
        clean_response = response_text[start_index:].strip()
        
        if not clean_response or clean_response == prompt:
             return "No pude generar una respuesta sobre eso. 쯇uedes preguntarme de otra forma?"
        
        return clean_response
    except Exception as e:
        print(f"Error durante la generaci칩n: {e}")
        return "Disculpa, tuve un problema al generar la respuesta."

def chatbot_interface(user_input, history):
    """Funci칩n que maneja la conversaci칩n y la interfaz de Gradio."""
    response = generate_response(user_input)
    return response

# --- 3. Lanzar la Interfaz de Gradio ---

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # 游뱄 Gu칤a Conversacional de Cultura y Gastronom칤a de Monterrey
        Preg칰ntame sobre la historia del Paseo Santa Luc칤a, el cabrito, o cualquier otro dato local.
        """
    )
    # Usamos gr.ChatInterface, que es m치s simple para un chatbot
    chatbot = gr.ChatInterface(
        fn=chatbot_interface,
        title="Gu칤a Compacta de MTY",
        chatbot=gr.Chatbot(height=500),
        textbox=gr.Textbox(placeholder="Escr칤beme tu pregunta sobre MTY", container=False, scale=7),
        submit_btn="Enviar",
        # El argumento 'clear_btn' causaba el error. El bot칩n se incluye autom치ticamente.
        # Agregamos 'type="messages"' para corregir el UserWarning.
        type="messages",
    )

if __name__ == "__main__":
    print("Iniciando interfaz de Gradio... (Recuerda ejecutar 'train.py' primero si el modelo no existe)")
    demo.launch()